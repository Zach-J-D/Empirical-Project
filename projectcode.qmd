---
title: 'TBD'
format:
    pdf:
        toc: true
        fontsize: 10pt
        geometry: margin=0.6in
---

# Importing the Data
```{python}
# Importing the relevant modules and tools necessary

import numpy as np
import pandas as pd
import statsmodels.api as sm

from pathlib import Path
import matplotlib.pyplot as plt 
from functools import reduce

from statsmodels.regression.linear_model import OLS
from statsmodels.tools import add_constant
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.outliers_influence import variance_inflation_factor


# Since the wage and interest rate data was monthly, have to adjust it
# Involves resampling this data into quarterly format for analysis later on
# For wages, took the mean of the period
# For interest rates, took the rate at the end of the quarter

wages_path = Path('Data') / 'wages.csv'
wages_df = pd.read_csv(wages_path)
wages_df['Date'] = pd.to_datetime(wages_df['Date'], format = 'mixed')
wages_df.set_index('Date', inplace = True)
quarterly_wages_df = wages_df.resample('QE-MAR').mean()
quarterly_wages_df.index = quarterly_wages_df.index.to_period('Q')

rates_path = Path('Data') / 'interest_rates.csv'
rates_df = pd.read_csv(rates_path)
rates_df['Date'] = pd.to_datetime(rates_df['Date'], format = 'mixed')
rates_df.set_index('Date', inplace = True)
quarterly_rates_df = rates_df.resample('QE-MAR').last()
quarterly_rates_df.index = quarterly_rates_df.index.to_period('Q')

# Creating the first dataframe, joining on the 'Date'column

first_df = pd.merge(quarterly_wages_df, quarterly_rates_df, on = ['Date'])


# Since the rest of the data was quarterly and in a similar format
# Wrote a function that could sort and create the dataframesy
# Takes file path, reads in data, much of the data was 2000 Q1
# However this isn't recognised and requires 2000-Q1 format instead
# If data was 2000-Q1, nothing is changed 

def create_df(dataset, column = 'Date', folder = 'Data'):
    dataset_path = Path(folder) / dataset
    try:
        new_df = pd.read_csv(dataset_path)
    except FileNotFoundError:
        print(f'File {dataset} not found in folder {folder}')
    if column in new_df.columns:
        new_df[column] = pd.PeriodIndex((new_df[column].str.replace(' ', '-')), freq = 'Q')
    return new_df


# Importing the rest of the data into dataframes

inflation_df = create_df('inflation.csv')
unemployment_df = create_df('unemployment.csv')
OECD_growth_df = create_df('OECD_growth.csv')
gvt_spending_df = create_df('gvt_spending.csv')


# Creating a dataframe with real (inflation adjusted) values in for future use 

real_variables_df = pd.merge(first_df, inflation_df, on = ['Date'])

real_variables_df['Real Wage Growth(%)'] = (real_variables_df['Wage Growth(%)'] - real_variables_df['Inflation(%)'])

real_variables_df['Real Interest Rate(%)'] = (real_variables_df['Bank Rate(%)'] - real_variables_df['Inflation(%)'])

real_variables_df

```

# The OLS Regression 
```{python}

# First going to gather all dataframes

all_dfs = [first_df, inflation_df, unemployment_df, gvt_spending_df, OECD_growth_df]

# Then perform a merge on the 'Date' column 

analysis_df = reduce(lambda left, right: pd.merge(left, right, on = ['Date'], how ='inner'), all_dfs)

ols_regression_df = analysis_df.drop('Date', axis = 1)

# Catergorising my variables and ensuring the columns exist

dep_var = 'Wage Growth(%)'
indep_var = ['Bank Rate(%)', 'Inflation(%)', 'Unemployment Rate(%)', 'OECD Economic Growth(%)', 'Gvt Expenditure Growth(%)']

assert dep_var in ols_regression_df.columns, "'Wage Growth(%)' is not recognsied"
for column in indep_var:
    assert column in ols_regression_df.columns, f"'{column}' is not recognised"

# Dropping the 4 NaN values at the start of the wage column 
# This is because the data starts at 2000 so there's obviously no data for the first year's quarters

ols_regression_df = ols_regression_df.dropna()

# Creating my Y and X values for the regression 

Y = ols_regression_df[dep_var]
X = ols_regression_df[indep_var]


# Runinng the OLS regression and printing the results

ols_regression = OLS(Y, add_constant(X)).fit(cov_type = 'HC1')  
print(ols_regression.summary())

```

## Validation Tests 
### Heteroskedasticity test
```{python}

# Running a heteroskedasticity test 

bp_test = het_breuschpagan(ols_regression.resid, ols_regression.model.exog)
bp_test

```

### Multicollinearity test
```{python}

# Running a test for multicollnearity

X_with_constant = add_constant(X)
vif_test = pd.DataFrame()
vif_test['Variable'] = X_with_constant.columns
vif_test['VIF'] = [variance_inflation_factor(X_with_constant, i)
                   for i in range(X_with_constant.shape[1])]


vif_test

```

### Bootstrapping Confidence Intervals
```{python}

# Setup for bootstrapping, including how many times to repeat the process

B = 3000
n = len(ols_regression_df)

# Creating an empty list to append to later

results_boot_list = []

# Writing a function that repeats the OLS as many times as specified
# Then, stores the coefficients

for i in range (B):
    boot_df = ols_regression_df.sample(n, replace = True)
    
    indep_boot = boot_df[indep_var]
    
    dep_boot = boot_df[dep_var]
    
    ols_boot = OLS(dep_boot, add_constant(indep_boot)).fit()
    
    results_boot_list.append(ols_boot.params.values)


# The dataframe with the initial bootstrapping results

results_boot_df = pd.DataFrame(results_boot_list, columns = ['const'] + indep_var)

# Getting my confidence intervals from the coefficients

CI_lower = results_boot_df.quantile(0.025)
CI_upper = results_boot_df.quantile(0.975)

# Table for the data so that the confidence intervals are columns

CI_boot = pd.concat([CI_lower, CI_upper], axis = 1)
CI_boot.columns = [' Bootstrap 0.025', ' Bootstrap 0.975']

# Getting the confidence intervals from my OLS for comparison

CI_ols = ols_regression.conf_int(alpha = 0.05)
CI_ols.columns = ['OLS 0.025', 'OLS 0.975']

# Joining the two datasets into one table

CI_merged = pd.concat([CI_boot, CI_ols], axis = 1)
CI_merged


```

The purpose of the bootstrapping was to estimate a sampling distribution for the regression coefficients without assuming a normal distribution for the residuals. By resampling the dataset and reestimating the OLS, it builds a distribution for the coefficients so I can empirically calculate the confidence intervals and compare them against the ones in the OLS output. This was to test how robust the OLS confidence intervals were.

As seen by the output above, the two sets of confidence intervals are very similar, with 4 of the 5 variables having both the upper and lower bound intervals within tight margins of difference. The largest being the lower bound of OECD Economic growth, which went from 0.218 to 0.3. However, none of the intervals changed in significance since none of the bootstrap contained 0 in their interval, as in the OLS. As such, the statistical significance of the variables in their relationship to wage growth did not change, and the bootstrapping results give credence to the validity of the original OLS output.

### Q-Q plot for normality checks
```{python}

# Plotting a Q-Q plot to test the normality of my residuals

sm.qqplot(ols_regression.resid, line='45', fit=True)
plt.title('Q-Q Plot for the Residuals')


```

The output from my Q-Q plot suggests that most of the residuals are normally distributed, as the data hugs the 45 degree line. While there is some deviation with the slightly heavy tails, which I already knew due to the kurtosis value, there's no strong skewness or persisting issues with the distribution. This gives more justification for me to use the OLS regression output to draw conclusions about the variables' causal realtionship with wage growth. 

