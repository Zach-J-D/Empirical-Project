---
title: '21st Century Real Wage Growth in the UK'
format:
    pdf:
        toc: true
        fontsize: 10pt
        geometry: margin=0.6in
---
# Introduction 

The purpose of this project is examine the relationships between wage growth and other macroeconomic variables in the UK in the 21st Century, and whether or not these variables exhibit a causal relationship with wage growth. To do this, I have collected data and built my own model of wage growth.
The variables I have decided to assess are: 
    - Bank rate
    - Inflation (CPIH)
    - Unemployment rate
    - Total government spending
    - OECD economic growth (1)

First, I shall take some variables individually and look at their relationship with wage growth, before later moving onto to deeper regression analysis to investigate the possibility of causality. I will also attempt to use the model to forecast 'future' wage growth.

*(1 - Clarifying that OECD economic growth is designed to act as a representative of foreign demand for both our goods and labour, and how this ultimately effects wages in the UK.)*  


# Importing the Data
```{python}
# Importing the relevant modules and tools necessary

import numpy as np
import pandas as pd
import seaborn as sns
import statsmodels.api as sm

from pathlib import Path
import matplotlib.pyplot as plt 
from functools import reduce

from statsmodels.regression.linear_model import OLS
from statsmodels.tools import add_constant

from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.outliers_influence import variance_inflation_factor

from scipy import stats
from statsmodels.tsa.stattools import grangercausalitytests

# Since the wage and interest rate data was monthly, have to adjust it

# Involves resampling this data into quarterly format for analysis later on

# For wages, took the mean of the period
# For interest rates, took the rate at the end of the quarter

wages_path = Path('Data') / 'wages.csv'
wages_df = pd.read_csv(wages_path)
wages_df['Date'] = pd.to_datetime(wages_df['Date'], format = 'mixed')

wages_df.set_index('Date', inplace = True)
quarterly_wages_df = wages_df.resample('QE-MAR').mean()
quarterly_wages_df.index = quarterly_wages_df.index.to_period('Q')

# Repeating for bank rate

rates_path = Path('Data') / 'interest_rates.csv'
rates_df = pd.read_csv(rates_path)
rates_df['Date'] = pd.to_datetime(rates_df['Date'], format = 'mixed')

rates_df.set_index('Date', inplace = True)
quarterly_rates_df = rates_df.resample('QE-MAR').last()
quarterly_rates_df.index = quarterly_rates_df.index.to_period('Q')

# Creating the first dataframe, joining on the 'Date'column

first_df = pd.merge(quarterly_wages_df, quarterly_rates_df, on = ['Date'])


# Since the rest of the data was quarterly and in a similar format

# Wrote a function that could sort and create the dataframesy
# Takes file path, reads in data, much of the data was 2000 Q1

# However this isn't recognised and requires 2000-Q1 format instead
# If data was 2000-Q1, nothing is changed 

def create_df(dataset, column = 'Date', folder = 'Data'):
    
    dataset_path = Path(folder) / dataset
    
    try:
        new_df = pd.read_csv(dataset_path)
    
    except FileNotFoundError:
        print(f'File {dataset} not found in folder {folder}')
    
    if column in new_df.columns:
        new_df[column] = pd.PeriodIndex((new_df[column].str.replace(' ', '-')), freq = 'Q')
    
    return new_df


# Importing the rest of the data into dataframes

inflation_df = create_df('inflation.csv')
unemployment_df = create_df('unemployment.csv')
OECD_growth_df = create_df('OECD_growth.csv')
gvt_spending_df = create_df('gvt_spending.csv')

```


## Creating DataFrames
```{python}
# Creating a dataframe with real (inflation adjusted) values in for future use 

real_variables_df = pd.merge(first_df, inflation_df, on = ['Date'])

real_variables_df['Real Wage Growth(%)'] = (real_variables_df['Wage Growth(%)'] - real_variables_df['Inflation(%)'])

real_variables_df['Real Interest Rate(%)'] = (real_variables_df['Bank Rate(%)'] - real_variables_df['Inflation(%)'])

# First going to gather all dataframes

all_dfs = [first_df, inflation_df, unemployment_df, gvt_spending_df, OECD_growth_df]

# Then perform a merge on the 'Date' column 

analysis_df = reduce(lambda left, right: pd.merge(left, right, on = ['Date'], how ='inner'), all_dfs)

analysis_df = analysis_df.dropna()

analysis_df

```

All of the data, apart from OECD economic growth, came from the ONS as they have reliable and extensive datasets, while the OECD economic growth data came from the OECD. 

As for the problems I faced in this process, there isn't free UK wage growth data that goes beyond 2000, and the ONS data was monthly whereas most other data collected on economic variables is quarterly or annual. This is a problem for being able to place the data side by side in a dataframe. Hence, I had to adjust the interest rate and wage growth data to a quarterly format. As well as this, almost all of the quarterly data had the date columns in the format YYYY QX, but only the format YYYY-QX is accepted in the pandas PeriodIndex. To deal with this, I wrote a function that sorted through the date columns replacing the blank space with a '-'. 

The final issue was that because the data for government spending and wages starts in 2000, there is no growth data for the first year, so those 2 variables have NaN values for 2000. Having done this, I was able to gather the data all into one dataframe as seen above. 

# How Wage Growth Moves With Key Variables

## Inflation 

```{python}

# Setting the size of the plot

plt.figure(figsize = (12, 10))

# Using seaborn to plot a scatterplot with a line of best fit

sns.regplot(
    x = analysis_df['Inflation(%)'],
    y = analysis_df['Wage Growth(%)'],
    line_kws = {'color': 'red', 'linewidth': 1.5}
)

# Giving it a title

plt.title('Wage Growth vs Inflation')
plt.axhline(0, color = 'green', linestyle = '--')
plt.show()

# Calculating the pearsons correlation coefficient

inflation_corr = analysis_df['Wage Growth(%)'].corr(analysis_df['Inflation(%)'])

# Printing the coefficient 

print(f'The pearsons correlation coefficient is {inflation_corr}')


```

This graph examines how wage growth moves with inflation, providing insight into the nature of their relationship. The line of best fit shows a visual representation of how wage growth increases as inflation increases, being upward sloping. I also calculated the pearsons correlation coefficient, which was 0.43. Both the pearsons statistic and the scatter plot evidence that the correlation is not a strong one. In the plot, there are values of wage growth ranging from -2% to 8% at roughly 2% inflation, the cluster of values are visibly spread in a more random manor than some economic theory would usualy suggest. This translates as that wage growth typically picks up when inflation increases, but not always and sometimes the opposite is true. 

This is because there are a host of other variables which can effect wage growth, and often inflation can't be the sole driving force of wage growth. For an example of high inflation not causing wage growth. Say an economy is experiencing a recession, this is often accompanied by higher unemployment and therefore workers having lower bargaining power. Then lets say there's an external supply-side shock which significantly raises costs for firms, this will lead to inflation, but due to the workers' lack of bargaining power, wages will not see even near the same increase until the economy has recovered. 



## Real Interest Rates
```{python}

plt.figure(figsize = (10, 8))

# Converting 'Date' column to datetime so it can plotted

real_variables_df['Date'] = pd.PeriodIndex(real_variables_df['Date'], freq = 'Q').to_timestamp()

# Looping through the columns in the dataframe to plot both time series together

for col in ['Real Wage Growth(%)', 'Real Interest Rate(%)']:
    plt.plot(real_variables_df['Date'], real_variables_df[col]\
        , label = col)


# Customising the graph

plt.title('Real Wage Growth and Real Interest Rates')
plt.xlabel('Year')
plt.ylabel('(%)')
plt.legend()
plt.axhline(0, color = 'black', linestyle = '--')
plt.show()

```

This figure showcases the time series of the real interest rate and real wage growth plotted against together. I explored the real variables as interest rates spent more than a decade at the zer lower bound, so by adjusting for inflation there is a much clearer picutre. This enables me to 1. See how the variables move together over time and 2. decipher any presence, and if so to what degreem, of lagged effects of the real interest rate on wage growth. 

When looking at the data, the two variables generally move in the same direction, frequently having peaks and troughs at the same time. This intuitively makes sense since when wage growth rises, it's often true that inflation is high too. As such, central banks provide forward guidance and then raise interest rates to lower inflation. Because firms and workers have expectations, they accordingly adjust and this leads to simultaneous peaks, as wage demands or price rises slow down in the period immediately after the interest rate hike. So the typical inverse relationship isn't strictly seen because of the time lag.




# The OLS Regression 
```{python}


# Dropping the 4 NaN values at the start of the wage column 
# This is because the data starts at 2000 so there's obviously no data for 
# the first year's quarters

ols_regression_df = analysis_df.copy()
ols_regression_df.dropna()

ols_regression_df = analysis_df.drop('Date', axis = 1)

# Catergorising my variables and ensuring the columns exist

dep_var = 'Wage Growth(%)'
indep_var = ['Bank Rate(%)', 'Inflation(%)', 'Unemployment Rate(%)', 'OECD Economic Growth(%)', 'Gvt Expenditure Growth(%)']

assert dep_var in ols_regression_df.columns, "'Wage Growth(%)' is not recognsied"
for column in indep_var:
    assert column in ols_regression_df.columns, f"'{column}' is not recognised"


# Creating my Y and X values for the regression 

Y = ols_regression_df[dep_var]
X = ols_regression_df[indep_var]


# Runinng the OLS regression and printing the results

ols_regression = OLS(Y, add_constant(X)).fit(cov_type = 'HC1')  
print(ols_regression.summary())

```

The regression results present some interesting ideas, but before those are discussed, I want to assess the validity of the model and address the issues the results raise. 

Yet, there are potential issues in the test statistics underneath the regressors. The condition number at 59 indicates high multicollinearity, but I will explore this further by calculating variance inflation factors for each of the regressors. The Durbin-Watson statistic, a measure of autocorrelation, is within the acceptable bounds of between 1.5 and 2.5. So there is some mild positive autocorrelation, which means the variables' errors are somewhat related to the errors at previous time periods, but not a degree that violates the assumption of independent error terms. This is very common amongst economic variables in a time series because of the cyclical nature of many economic measures.

The OLS assumption that residuals follow a normal distribution is brought into question, which are assessed by the Omnibus and Jarque-Bera (JB) statistics. Taking alpha = 0.05, the JB p-value is statistically significant at 0.0442, suggesting the residuals are not normally distributed. While the Omnibus p-value is not significant at 0.068. However, in combination with the JB p-value, the 0.068 cannot justify the residuals are normally distributed. The kurtosis value of 4.151 suggests that the residuals have fatter tails, or in other words there are outliers present in the data. This could also account for the doubt in the distribution of the residuals. 

In order to investigate this further to assess to what degree normality is violated, I will provide a Quantile-Quantile (Q-Q) plot to visually examine the distribution of the residuals. Normality being violated can lead to unreliable confidence intervals and p-values, so to counter this I will also bootstrap confidence intervals and compare against the ones in the OLS regression to assess the accuracy.


## Validation Tests 

### Q-Q plot for normality checks
```{python}

# Plotting a Q-Q plot to test the normality of my residuals

sm.qqplot(ols_regression.resid, line='45', fit=True)
plt.title('Q-Q Plot for the Residuals')


```

The output from my Q-Q plot suggests that most of the residuals are normally distributed, as the data hugs the 45 degree line. While there is some deviation with the slightly heavy tails, which was suggested by the kurtosis value, there's no strong skewness or persisting issues with the distribution. This gives more justification for me to use the OLS regression output to draw conclusions about the variables' causal realtionship with wage growth. 


### Bootstrapping Confidence Intervals
```{python}

# Setup for bootstrapping, including how many times to repeat the process

B = 1000
n = len(ols_regression_df)

# Creating an empty list to append to later

results_boot_list = []

# Writing a function that repeats the OLS as many times as specified
# Then, stores the coefficients

for i in range (B):
    boot_df = ols_regression_df.sample(n, replace = True)
    
    indep_boot = boot_df[indep_var]
    
    dep_boot = boot_df[dep_var]
    
    ols_boot = OLS(dep_boot, add_constant(indep_boot)).fit()
    
    results_boot_list.append(ols_boot.params.values)


# The dataframe with the initial bootstrapping results

results_boot_df = pd.DataFrame(results_boot_list, columns = ['const'] + indep_var)

# Getting my confidence intervals from the coefficients

CI_lower = results_boot_df.quantile(0.025)
CI_upper = results_boot_df.quantile(0.975)

# Table for the data so that the confidence intervals are columns

CI_boot = pd.concat([CI_lower, CI_upper], axis = 1)
CI_boot.columns = [' Bootstrap 0.025', ' Bootstrap 0.975']

# Getting the confidence intervals from my OLS for comparison

CI_ols = ols_regression.conf_int(alpha = 0.05)
CI_ols.columns = ['OLS 0.025', 'OLS 0.975']

# Joining the two datasets into one table

CI_merged = pd.concat([CI_boot, CI_ols], axis = 1)
CI_merged


```

The purpose of the bootstrapping was to estimate a sampling distribution for the regression coefficients without assuming a normal distribution for the residuals. By resampling the dataset and reestimating the OLS, it builds a distribution for the coefficients so I can empirically calculate the confidence intervals and compare them against the ones in the OLS output. This was to test how robust the OLS confidence intervals were in light of the issues with the normality in the distribution of the residuals.

As seen by the output above, the two sets of confidence intervals are very similar, with 4 of the 5 variables having both the upper and lower bound intervals within tight margins of difference. The largest being the lower bound of OECD Economic growth, which went from 0.22 to 0.3. However, none of the intervals changed in significance since none of the bootstrapped intervals contained 0 in their interval, as in the OLS. As such, the statistical significance of the variables in their relationship to wage growth did not change, and the bootstrapping results give credence to the validity of the original OLS output. 

### Heteroskedasticity test
```{python}

# Running a heteroskedasticity test 

bp_test = het_breuschpagan(ols_regression.resid, ols_regression.model.exog)
bp_test

```

This is the Breush-Pagan test for heteroskedasticity. Heteroskedasticity is where the error terms do not have a constant variance, and if found to be present would violate one of the key assumptions of OLS. However, with a the p-value of 0.12, there is insufficient evidence to reject the null hypothesis that the error terms' variance are homoskedastic. As such, the assumption is not violated in this model.



### Multicollinearity test
```{python}

# Running a test for multicollnearity

X_with_constant = add_constant(X)
vif_test = pd.DataFrame()
vif_test['Variable'] = X_with_constant.columns
vif_test['VIF'] = [variance_inflation_factor(X_with_constant, i)
                   for i in range(X_with_constant.shape[1])]


vif_test

```

Next is calculating the variance inflation factors for the regressors, so that I can check whether multicollinearity is indeed an issue in this model. A value of 1 is no multicollinearity at all, and values above 5 indicate high multicollinearity. This is cause for concern for the reliability of the model as strong correlations between regressors can inflate standard errors and lead to unstable coefficients. In this OLS regression, none of the regressors have a VIF higher than 2, showing that this model does not suffer from problematic multicollinearity, enforcing the reliability of the regression's results. 


## Discussion of the OLS Results 

Now, having evaluated the quality of the model, the results themselves can be discussed with relative confidence. First, at first glance we can be confident that the model is a good fit as both the R-squared and adjusted R-squared, 0.776 and 0.763 respectively, are above 0.7 which is often considered a rule of thumb for a good fit. All of the 5 regressors are statistically significant in the OLS regression, with all the p-values being < 0.05. Meaning all of the chosen variables have a causal relationship with wage growth. 

Most of the signs of the coefficients are as expected, for example unemployment where its coefficient is -0.3. This means a 1% rise in unemployment decreases wage growth by -0.3%, which makes sense since as the labour market slackens and workers lose bargaining power, you would expect wage growth to fall. Wage growth rises due to government spending, explained by direct public sector job creation / investment, or indirectly through crowding in private investment. For OECD economic growth, this is mostly a result of growing foreign demand driving UK export sectors' wages. Or specifically for the UK given our extensive financial services, other countries' growth benefits our economy as they need our services more. 

Inflation is as to be expected, with a 0.4 coefficient. Rising inflation erodes the purchasing power of workers, who in response demand higher wages to maintain their standard of living. 

However, the Bank Rate coefficient is not as would typically be in standard economic theory, at 0.24. That is, that higher interest rates decrease wage growth as more costly borrowing and a larger reward for saving incentivise firms to cut back on investment and slow wage growth. Here, the Bank Rate increases wage growth. I think this is best explained by the lagged response of interest rates. Rates are lowered to stimulate the economy, commonly when wage growth is lower. The lower rate leads to more borrowing and investment, which increases economic growth and is accompanied by higher wages and eventually inflation as the economy overheats. So, to combat the higher inflation the Bank of England raises the Bank Rate, but due to the lagged nature of the transmission mechanism, in the data itself the Bank Rate increases when wage growth does.


# Setup For ARIMAX
## First Differencing and ADF Tests
```{python}

from statsmodels.tsa.stattools import adfuller

# Creating a new dataframe to use for first-differencing and VECM tests

copy_df = ols_regression_df.copy()
stationary_df = copy_df.diff().dropna()

# Runs the ADF(Augmented Dickey-Fuller) test on every column

# Returns the results into a previously empty list
# Purpose is to make sure data is stationary after first-differencing

def adf_test(df):
    adf_results = {}
    
    for column in df.columns:
        result = adfuller(df[column].dropna(), maxlag = 5)
        
        adf_results[column] = {
            'ADF Statistic': result[0],
            'p-value': result[1],
            'Lags Used': result[2],
            'Number of Observations Used': result[3],
            'Critical Values': result[4]
        }
        
    return adf_results

# Running it on the stationary dataframe

adf_results = adf_test(stationary_df)

# Printing out the results from the ADF test 
# Can inspect lags used and the relevant t-stats and p-values

for column, result in adf_results.items():
    print(f"\nResults for {column}:")
    
    print(f"ADF Statistic: {result['ADF Statistic']}")
    
    print(f"p-value: {result['p-value']}")
    
    print(f"Lags Used: {result['Lags Used']}")
    
```

Since ARIMAX requires the data to be stationary, and my data was not stationary having tested it, I needed to first difference the data to achieve stationarity. Once I differenced the data, I ran the Augmented-Dickey Fuller test to confirm. As can be seen from the output, all of the p-values for the variables are < 0.05, and are therefore now stationary and ready to be used in ARIMAX.




```{python}

# For forecasting purposes  

stationary_df['Date'] = analysis_df['Date']
stationary_df = stationary_df.set_index('Date')

stationary_df = stationary_df.asfreq(stationary_df.index.freq)
stationary_df.index = stationary_df.index.to_timestamp()
```

# ARIMAX - Testing Predictive Power of the Model

```{python}

from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error

# Set the period I'm going to be forecasting

testing_timeframe = 8

# Create the test and train datasets

arimax_train = stationary_df[:-testing_timeframe]
arimax_test  = stationary_df[-testing_timeframe:]


# Define the exogenous variables
exog_arimax_train = arimax_train[['Bank Rate(%)',\
    'Inflation(%)','Unemployment Rate(%)',\
    'Gvt Expenditure Growth(%)', 'OECD Economic Growth(%)']]

exog_arimax_test  = arimax_test[['Bank Rate(%)',\
    'Inflation(%)','Unemployment Rate(%)',\
    'Gvt Expenditure Growth(%)', 'OECD Economic Growth(%)']]

# Creates and fits the model with the appropriate parameters
arimax_model = ARIMA(arimax_train['Wage Growth(%)'],  
                    order = (2,0,1),
                    exog = exog_arimax_train
                   ).fit()

print(arimax_model.summary())

# Creates forecasts on training set 

forecasts_on_train = arimax_model.predict()

# Creates forecasts on test set

forecasts_on_test  = arimax_model.forecast(len(arimax_test), exog = exog_arimax_test)

```

## Diagnostic Check
```{python}

arimax_model.plot_diagnostics(figsize = (14,10))
plt.show()


```

Next, I wanted to check the validity of the model to assess the usefulness of the results. As part of the ARIMAX output, there is provided tests for heteroskedasticity and normality, alongside information on kurtosis and skew. Heteroskedasticity is not an issue in this model, neither is autocorrelation as seen by the Ljung-Box p-value. Kurtosis is higher at 4.24, as was in the OLS, and normality is again being questioned with the Jarque-Bera p-value. So, to investigate further I produced a diagnostic plot.

In the diagnostic plot, the top left showcases the residuals for wage growth, where you can see that there are no obvious trends or seasonality - a good indicator for the validity of the model. 

The bottom left plot is a Q-Q plot to assess the normality of the residuals. Similar to the earlier OLS regression, most of the data is along the diagonal line, suggesting the residuals are normally distributed, however there are deviations at the tails just as before. This was implied by the kurtosis statistic, and is expected with the outliers i.e. Covid-19 Pandemic, Global Financial Crisis etc that are present in the dataset.

In the top right there is a histogram alonside a normal distribution to again test normality. With similar results, the distribution is very close to normal but there are heavy tails.

The correlogram in the bottom right tests for autocorrelation in the residuals, and as can be seen almost all the lags fall within the shaded area, which is a 95% confidence interval, so it can be assumed that there's no significant autocorrelation. 

## Forecasting
```{python}

# Plotting the train and test data against their corresponding forecasts

plt.figure(figsize=(16,4))
plt.plot(arimax_train['Wage Growth(%)'], label="Actual")
plt.plot(forecasts_on_train, label="Predicted")

plt.title('The Forecast of the Training Set vs the Actual Data')
plt.xlabel('Year')
plt.ylabel('% Change')
plt.legend()

# Repeating for test data, where the model is really tested
plt.figure(figsize=(16,4))
plt.plot(arimax_test['Wage Growth(%)'], label="Actual")
plt.plot(forecasts_on_test, label="Predicted")

plt.title('The Forecast for the Unseen Test Data')
plt.xlabel('Date')
plt.ylabel('% Change')
plt.legend()
plt.show()


```

Since this project's goal is to investigate causality, the primary purpose behind forecasting with this model is to test how accurate the model is, lending help to assess how much the other results can be trusted.

Both in the training set and unseen test data, the model performed well in predicting wage growth. In the training set, the predictions followed the shape, and often magnitude, of the actual wage growth data. While in the test set, of the 8 data points, the model was able to accurately predict 7 of them, being wrong only in 2023 Q4. This provides evidence that the results of the ARIMAX can indeed be trusted, as the forecasting was very precise. 

If forecasting was the primary goal of a project with this dataset, it could be used to that end to predict wage growth in the future.

## Discussing the ARIMAX Results, and Comparing to the OLS and Granger-Causality

From the ARIMAX results, we can see the coefficients are notably different to that of the OLS. While inflation and OECD economic growth remained significant, inflation is no longer significant, and doesn't appear to have a clear relationship with wage growth. Unemployment and government spending are not statistically significant either despite being closer to significance. This suggests that once time dynamics and previous wage growth are accounted for, these variables lose their causality. 

The autoregressive componenets (ar.L1, ar.L2) provide great insight into wage growth. Their statistical significance means wage growth is influenced by its past values, and with coefficients of -1.48 and -0.72 this indicates that a 1% increase in wage growth last quarter leads to a -1.48% decrease this quarter, or a -0.72% decrease if the growth was 2 quarters ago. So, wage growth oscillates a lot, which was seen earlier in the plot of real wage growth and real interest rates, but this oscillation can help to predict future growth as well. This dynamic is what the OLS fails to capture, and likely inflated the significance of variables in OLS. 

The other new variable is the moving average term (ma.L1), which reflects the impact of past errors on the model. With a coefficient of 0.95, this means that if there was an unexpected positive error in the previous quarter, the MA term will increase wage growth this quarter. This is something else the OLS doesn't capture, and why the ARIMAX is critical to this analysis. 




