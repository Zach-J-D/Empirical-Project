---
title: 'TBD'
format:
    pdf:
        toc: true
        fontsize: 10pt
        geometry: margin=0.6in
---

# Importing the Data
```{python}
# Importing the relevant modules and tools necessary

import numpy as np
import pandas as pd

from pathlib import Path
import matplotlib as plt 
from functools import reduce

from statsmodels.regression.linear_model import OLS
from statsmodels.tools import add_constant
from statsmodels.stats.diagnostic import het_breuschpagan
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Since the wage and interest rate data was monthly, have to adjust it
# Involves resampling this data into quarterly format for analysis later on
# For wages, took the mean of the period
# For interest rates, took the rate at the end of the quarter

wages_path = Path('Data') / 'wages.csv'
wages_df = pd.read_csv(wages_path)
wages_df['Date'] = pd.to_datetime(wages_df['Date'], format = 'mixed')
wages_df.set_index('Date', inplace = True)
quarterly_wages_df = wages_df.resample('QE-MAR').mean()
quarterly_wages_df.index = quarterly_wages_df.index.to_period('Q')

rates_path = Path('Data') / 'interest_rates.csv'
rates_df = pd.read_csv(rates_path)
rates_df['Date'] = pd.to_datetime(rates_df['Date'], format = 'mixed')
rates_df.set_index('Date', inplace = True)
quarterly_rates_df = rates_df.resample('QE-MAR').last()
quarterly_rates_df.index = quarterly_rates_df.index.to_period('Q')

# Creating the first dataframe, joining on the 'Date'column

first_df = pd.merge(quarterly_wages_df, quarterly_rates_df, on = ['Date'])


# Since the rest of the data was quarterly and in a similar format
# Wrote a function that could sort and create the dataframesy
# Takes file path, reads in data, much of the data was 2000 Q1
# However this isn't recognised and requires 2000-Q1 format instead
# If data was 2000-Q1, nothing is changed 

def create_df(dataset, column = 'Date', folder = 'Data'):
    dataset_path = Path(folder) / dataset
    try:
        new_df = pd.read_csv(dataset_path)
    except FileNotFoundError:
        print(f'File {dataset} not found in folder {folder}')
    if column in new_df.columns:
        new_df[column] = pd.PeriodIndex((new_df[column].str.replace(' ', '-')), freq = 'Q')
    return new_df


# Importing the rest of the data into dataframes

inflation_df = create_df('inflation.csv')
RoR_df = create_df('rate_of_return.csv')
productivity_df = create_df('productivity.csv')
unemployment_df = create_df('unemployment.csv')
OECD_growth_df = create_df('OECD_growth.csv')
investment_df = create_df('investment_growth.csv')


# Creating a dataframe with real (inflation adjusted) values in for future use 

real_variables_df = pd.merge(first_df, inflation_df, on = ['Date'])

real_variables_df['Real Wage Growth(%)'] = (real_variables_df['Wage Growth(%)'] - real_variables_df['Inflation(%)'])

real_variables_df['Real Interest Rate(%)'] = (real_variables_df['Bank Rate(%)'] - real_variables_df['Inflation(%)'])

real_variables_df

```


# The OLS Regression 
```{python}

# First going to gather all dataframes

all_dfs = [first_df, inflation_df, RoR_df, unemployment_df, productivity_df, OECD_growth_df, investment_df]

# Then perform a merge on the 'Date' column 

analysis_df = reduce(lambda left, right: pd.merge(left, right, on = ['Date'], how ='inner'), all_dfs)

ols_regression_df = analysis_df.drop('Date', axis = 1)

# Catergorising my variables and ensuring the columns exist

dep_var = 'Wage Growth(%)'
indep_var = ['Bank Rate(%)', 'Inflation(%)', 'Net Rate of Return(%)', 'Unemployment Rate(%)', 'Output per Hour Growth(%)', 'OECD Economic Growth(%)', 'Business Investment Growth(%)']

assert dep_var in ols_regression_df.columns, "'Wage Growth(%)' is not recognsied"
for column in indep_var:
    assert column in ols_regression_df.columns, f"'{column}' is not recognised"

# Dropping the 4 NaN values at the start of the wage column 
# This is because the data starts at 2000 so there's obviously no data for the first year's quarters

ols_regression_df = ols_regression_df.dropna()

# Creating my Y and X values for the regression 

Y = ols_regression_df[dep_var]
X = ols_regression_df[indep_var]


# Runinng the OLS regression and printing the results

ols_regression = OLS(Y, add_constant(X)).fit(cov_type = 'HC1')  
print(ols_regression.summary())

```

# Validation Tests
```{python}

# Running a heteroskedasticity test 

bp_test = het_breuschpagan(ols_regression.resid, ols_regression.model.exog)

# Running a test for correlation amongst the variables

X_with_constant = add_constant(X)
vif_test = pd.DataFrame()
vif_test['Variable'] = X_with_constant.columns
vif_test['VIF'] = [variance_inflation_factor(X_with_constant, i)
                   for i in range(X_with_constant.shape[1])]


print(vif_test)
print(bp_test)


```

# OLS regression with interactions

```{python}

# To have desired interaction variables, need to centre the relevant variables to reduce multicollinearity 

def centre_columns(df, columns):
    ols_centred_df = ols_regression_df.copy()
    for column in columns: 
        if column in ols_centred_df.columns:
            ols_centred_df[column] = ols_centred_df[column] - ols_centred_df[column].mean()
        else:
            print('Column not found')
    return ols_centred_df

inter_columns = ['Bank Rate(%)', 'Inflation(%)', 'Net Rate of Return(%)', 'Unemployment Rate(%)']

ols_interaction_df = centre_columns(ols_regression_df, inter_columns)

# Creating the interaction variable 

ols_interaction_df['RoR_unemployment'] = ols_interaction_df['Net Rate of Return(%)'] * ols_interaction_df['Unemployment Rate(%)'] 

ols_interaction_df['Bank_inflation'] = ols_interaction_df['Bank Rate(%)'] * ols_interaction_df['Inflation(%)']

# Editing the independent variable list to include the interaction effect

indep_var_interaction = ['Bank Rate(%)', 'Inflation(%)', 'Net Rate of Return(%)', 'Unemployment Rate(%)', 'Output per Hour Growth(%)', 'OECD Economic Growth(%)', 'Business Investment Growth(%)', 'RoR_unemployment', 'Bank_inflation']

X_interaction = ols_interaction_df[indep_var_interaction]

# Running the new OLS regression

ols_regression_interaction = OLS(Y, add_constant(X_interaction)).fit(cov_type = 'HC1') 
print(ols_regression_interaction.summary())

```

# Updated Validation Tests
```{python}

# Running a heteroskedasticity test 

bp_test_inter = het_breuschpagan(ols_regression_interaction.resid, ols_regression_interaction.model.exog)

# Running a test for correlation amongst the variables

X_with_constant_inter = add_constant(X_interaction)
vif_test_inter = pd.DataFrame()
vif_test_inter['Variable'] = X_with_constant_inter.columns
vif_test_inter['VIF'] = [variance_inflation_factor(X_with_constant_inter, i)
                   for i in range(X_with_constant_inter.shape[1])]


print(vif_test_inter)
print(bp_test_inter)


```

# Cointegration tests
```{python}

import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.vector_ar.vecm import coint_johansen


johansen_test = coint_johansen(ols_regression_df, det_order = 0, k_ar_diff = 1)

trace_stat = johansen_test.lr1
eigenvalue_stat = johansen_test.lr2

print(f'Trace statistic: {trace_stat}')
print(f'Max eigenvalue statistic: {eigenvalue_stat}')
print(f'Critical value: {johansen_test.cvt}')

